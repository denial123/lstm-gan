{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cgan_colab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKJD+wZ+m5ksk5vuXPEsRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denial123/lstm-gan/blob/master/cgan_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InXp2SHIvGEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3a44dc0-3bae-4e90-dc8e-99ca17876d14"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model, model_from_json\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import asarray\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "\n",
        "def get_notes(dir=\"love_simple/*.mid\"):\n",
        "    \"\"\" Get all the notes and chords from the midi files \"\"\"\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(dir):\n",
        "        midi = converter.parse(file)\n",
        "\n",
        "        print(\"Parsing %s\" % file)\n",
        "\n",
        "        notes_to_parse = None\n",
        "\n",
        "        try:  # file has instrument parts\n",
        "            s2 = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = s2.parts[0].recurse()\n",
        "        except:  # file has notes in a flat structure\n",
        "            notes_to_parse = midi.flat.notes\n",
        "\n",
        "        for element in notes_to_parse:\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return notes\n",
        "\n",
        "\n",
        "def prepare_sequences(notes, n_vocab, label_number=0):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # Create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    #network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    network_input = asarray(network_input)\n",
        "    # Normalize input between -1 and 1\n",
        "    network_input = (network_input - float(n_vocab) / 2) / (float(n_vocab) / 2)\n",
        "    print(\"network output before categorical\", network_output)\n",
        "    #network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    label = []\n",
        "    for i in range(0, n_patterns):\n",
        "        label.append(label_number)\n",
        "    #label = np_utils.to_categorical(label, num_classes=2)\n",
        "    print(\"network_input\", network_input)\n",
        "    print(len(label))\n",
        "    #print(\"label\",label)\n",
        "\n",
        "\n",
        "    # Versuch ohne eckige Klammern\n",
        "    return (network_input, label, network_output)\n",
        "\n",
        "\n",
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item[0]\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.show('text')\n",
        "    # show MuseScore\n",
        "    #midi_stream.show()\n",
        "    # einkommentieren, um midi file zu speichern\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
        "\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self, rows, load_model=False):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 1000\n",
        "        self.disc_loss = []\n",
        "        self.disc_loss_real = []\n",
        "        self.disc_loss_fake = []\n",
        "        self.gen_loss = []\n",
        "        self.d_acc_real = []\n",
        "        self.d_acc_fake = []\n",
        "        self.n_classes = 2\n",
        "\n",
        "        self.model_save_dir = 'cgan_saved_model'\n",
        "\n",
        "        if load_model:\n",
        "            self.load_model()\n",
        "        else:\n",
        "            self.init_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Load and compile the discriminator\n",
        "        self.discriminator = self.load_keras_model(\"cdiscriminator_model\")\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Load the generator\n",
        "        self.generator = self.load_keras_model(\"cgenerator_model\")\n",
        "        # Get noise and label input from generator model\n",
        "        gen_noise, gen_label = self.generator.input\n",
        "        # Get output sequence from the generator model\n",
        "        generated_seq = self.generator.output\n",
        "\n",
        "        # [ucgan] The generator takes noise as input and generates imgs\n",
        "        #z = Input(shape=(self.latent_dim,))\n",
        "        #generated_seq = self.generator(z)\n",
        "\n",
        "        # Connect output sequence and label input from generator as inputs to discriminator\n",
        "        gan_output = self.discriminator([generated_seq, gen_label])\n",
        "\n",
        "        # [ucgan] The discriminator takes generated images as input and determines validity = gan_output\n",
        "        # validity = self.discriminator(generated_seq)\n",
        "        # self.combined = Model(z, validity)\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = self.load_keras_model(\"ccombined_model\")\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "        ###\n",
        "\n",
        "    def init_model(self):\n",
        "        #in image example: method \"define gan()\"\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "        # Get noise and label input from generator model\n",
        "        gen_noise, gen_label = self.generator.input\n",
        "\n",
        "        # Get output sequence from the generator model\n",
        "        generated_seq = self.generator.output\n",
        "\n",
        "        # [ucgan] The generator takes noise as input and generates note sequences\n",
        "        #z = Input(shape=(self.latent_dim,))\n",
        "        #generated_seq = self.generator(z)\n",
        "\n",
        "        # Connect output sequence and label input from generator as inputs to discriminator\n",
        "        gan_output = self.discriminator([generated_seq,gen_label])\n",
        "\n",
        "        # [ucgan] The discriminator takes generated images as input and determines validity = gan_output\n",
        "        #validity = self.discriminator(generated_seq)\n",
        "        # self.combined = Model(z, validity)\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model([gen_noise, gen_label], gan_output)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_discriminator(self, in_shape=(100,1)):\n",
        "\n",
        "        # label input\n",
        "        in_label = Input(shape=(1,))\n",
        "        # embedding for categorical input\n",
        "        li = Embedding(self.n_classes, 50)(in_label)\n",
        "        # scale up to seq_length (100) with linear activation\n",
        "        li = Dense(self.seq_length)(li)\n",
        "        # reshape to additional channel\n",
        "        li = Reshape(self.seq_shape)(li)\n",
        "\n",
        "        # sequence input\n",
        "        in_seq = Input(shape=self.seq_shape)\n",
        "        # concat label as a channel\n",
        "        merge = Concatenate()([in_seq, li])\n",
        "\n",
        "        fe = LSTM(512, return_sequences=True)(merge)\n",
        "        fe = Bidirectional(LSTM(512))(fe)\n",
        "        fe = Dense(512)(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        fe = Dense(256)(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # output\n",
        "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\n",
        "        # define model\n",
        "        model = Model([in_seq, in_label], out_layer)\n",
        "        model.summary()\n",
        "        #plot_model(model, to_file='cdiscriminator_plot.png', show_shapes=True, show_layer_names=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "        ### from image example\n",
        "        # downsample\n",
        "        #fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
        "        #fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        #fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
        "        #fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # flatten feature maps\n",
        "        #fe = Flatten()(fe)\n",
        "        # dropout\n",
        "        #fe = Dropout(0.4)(fe)\n",
        "        # output\n",
        "        #out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "        # define model\n",
        "        #model = Model([in_image, in_label], out_layer)\n",
        "        # compile model\n",
        "        #opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "        #model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        #return model\n",
        "        ### end-- from image example\n",
        "\n",
        "        #-------old Sequential model\n",
        "        #model = Sequential()\n",
        "        #model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
        "        #model.add(Bidirectional(LSTM(512)))\n",
        "        #model.add(Dense(512))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dense(256))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dense(1, activation='sigmoid'))\n",
        "        #model.summary()\n",
        "        #plot_model(model, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "        #seq = Input(shape=self.seq_shape)\n",
        "        #validity = model(seq)\n",
        "\n",
        "        #return Model(seq, validity)\n",
        "        #end-- old Sequential model\n",
        "\n",
        "    def build_generator(self):\n",
        "        # label input\n",
        "        in_label = Input(shape=(1,))\n",
        "        # embedding for categorical input\n",
        "        li = Embedding(self.n_classes, 50)(in_label)\n",
        "\n",
        "        #### testen, was besser geht\n",
        "        #(A) scale up to latent_dim (1000) with linear activation\n",
        "        #li = Dense(self.latent_dim)(li)\n",
        "        #li = Reshape((self.latent_dim,))(li)\n",
        "        #(B)\n",
        "        li = Dense(1)(li)\n",
        "        li = Reshape((1,))(li)\n",
        "\n",
        "        in_lat = Input(shape=(self.latent_dim,))\n",
        "        merge = Concatenate()([in_lat, li])\n",
        "        gen = Dense(256)(merge)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = BatchNormalization(momentum=0.8)(gen)\n",
        "        gen = Dense(512)(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = BatchNormalization(momentum=0.8)(gen)\n",
        "        gen = Dense(1024)(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = BatchNormalization(momentum=0.8)(gen)\n",
        "        out_layer = Dense(np.prod(self.seq_shape), activation='tanh')(gen)\n",
        "        out_layer = Reshape(self.seq_shape)(out_layer)\n",
        "\n",
        "        # define model\n",
        "        model = Model([in_lat, in_label], out_layer)\n",
        "        model.summary()\n",
        "        #plot_model(model, to_file='cgenerator_plot.png', show_shapes=True, show_layer_names=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "        ###image conditional\n",
        "        # label input\n",
        "        #in_label = Input(shape=(1,))\n",
        "        # embedding for categorical input\n",
        "        #li = Embedding(n_classes, 50)(in_label)\n",
        "        # linear multiplication\n",
        "        #n_nodes = 7 * 7\n",
        "        #li = Dense(n_nodes)(li)\n",
        "        # reshape to additional channel\n",
        "        #li = Reshape((7, 7, 1))(li)\n",
        "        # image generator input\n",
        "        #in_lat = Input(shape=(latent_dim,))\n",
        "        # foundation for 7x7 image\n",
        "        #n_nodes = 128 * 7 * 7\n",
        "        #gen = Dense(n_nodes)(in_lat)\n",
        "        #gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        #gen = Reshape((7, 7, 128))(gen)\n",
        "        # merge image gen and label input\n",
        "        #merge = Concatenate()([gen, li])\n",
        "        # upsample to 14x14\n",
        "        #gen = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(merge)\n",
        "        #gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # upsample to 28x28\n",
        "        #gen = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(gen)\n",
        "        #gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # output\n",
        "        #out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
        "        # define model\n",
        "        #model = Model([in_lat, in_label], out_layer)\n",
        "        #return model\n",
        "        ##--end image conditional\n",
        "        ## old Sequential model\n",
        "        #model = Sequential()\n",
        "        #model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(BatchNormalization(momentum=0.8))\n",
        "        #model.add(Dense(512))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(BatchNormalization(momentum=0.8))\n",
        "        #model.add(Dense(1024))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(BatchNormalization(momentum=0.8))\n",
        "        #model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        #model.add(Reshape(self.seq_shape))\n",
        "        #model.summary()\n",
        "        #plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "        #noise = Input(shape=(self.latent_dim,))\n",
        "        #seq = model(noise)\n",
        "\n",
        "        #return Model(noise, seq)\n",
        "        #end--old Sequential model\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50, model_save_interval=50):\n",
        "\n",
        "        # Load and convert the data\n",
        "        dirs = [\"love_simple/*.mid\", \"anger_simple/*.mid\"]\n",
        "        X_train = []\n",
        "        x_labels = []\n",
        "        y_train = []\n",
        "\n",
        "        dir_count = 0\n",
        "        for dir in dirs:\n",
        "            notes = get_notes(dir)\n",
        "            #### ausprobieren Häufigkeit der Noten anzugeben\n",
        "            z = []\n",
        "            for x in range(len(notes)):\n",
        "                z.append(notes[x])\n",
        "\n",
        "            plt.hist(z)\n",
        "\n",
        "            plt.xlabel(\"Noten\")\n",
        "\n",
        "            plt.ylabel(\"Häufigkeit\")\n",
        "\n",
        "            #plt.show()\n",
        "            ####\n",
        "            n_vocab = len(set(notes))\n",
        "            ### hier labels mit übergeben\n",
        "            X_train_part, x_labels_part, y_train_part = prepare_sequences(notes, n_vocab, dir_count)\n",
        "            dir_count +=1\n",
        "            #X_train.append(X_train_part)\n",
        "            for xt in X_train_part:\n",
        "                X_train.append(xt)\n",
        "            ### label einzeln anhängen\n",
        "            for l in x_labels_part:\n",
        "                x_labels.append(l)\n",
        "            for yt in y_train_part:\n",
        "                y_train.append(yt)\n",
        "        ###\n",
        "        # Labels sind richtig umgewandelt\n",
        "        # bei X_train passt die shape noch nicht! nochmal angucken!\n",
        "        ###\n",
        "        # Reshape the input into a format compatible with LSTM layers\n",
        "        sequence_length = 100\n",
        "        X_train = np.reshape(X_train, (len(x_labels), sequence_length, 1))\n",
        "        #x_labels = asarray(x_labels)\n",
        "        x_labels = np.reshape(x_labels, (len(x_labels)))\n",
        "        print(\"x_labels\", x_labels)\n",
        "        y_train = np_utils.to_categorical(y_train)\n",
        "        print(\"X_train\", X_train)\n",
        "        print(\"len xtrain\", len(X_train))\n",
        "        print(len(X_train[0]))\n",
        "        print(len(X_train[0][0]))\n",
        "        print(len(X_train[1]))\n",
        "        print(len(X_train[1][0]))\n",
        "        #print(\"labels as categorical\",x_labels)\n",
        "        print(\"len labels\",len(x_labels))\n",
        "        #print(\"y_train\",y_train)\n",
        "\n",
        "\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        #### muss noch auf den Server übertragen werden #####\n",
        "        # One-sided label smoothing\n",
        "        #real = np.full((batch_size, 1), 0.9)\n",
        "\n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "            # ---------------------\n",
        "            # Training the discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_seqs = X_train[idx]\n",
        "            #print(\"real_seqs\",real_seqs)\n",
        "            #print(\"len real seqs\", len(real_seqs))\n",
        "            labels_real = x_labels[idx]\n",
        "            #print(\"labels_real\",labels_real)\n",
        "            #print(\"shape\", np.shape(labels_real))\n",
        "            #labels = labels[idx]\n",
        "            ### hier muss noch label zum network input der note sequences hinzugefügt werden ###\n",
        "\n",
        "            # Noise as input for the generator = generator points in latent space\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            # generate labels\n",
        "            noise_labels = randint(0, self.n_classes, batch_size)\n",
        "\n",
        "            # Generate a batch of new note sequences = predict outputs\n",
        "            gen_seqs = self.generator.predict([noise,noise_labels])\n",
        "\n",
        "            # Train the discriminator\n",
        "            ### d_loss_real: hier müssen noch labels zu den real_seqs hinzugefügt werden\n",
        "            d_loss_real = self.discriminator.train_on_batch([real_seqs, labels_real], real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_seqs, noise_labels], fake)\n",
        "            #print(self.discriminator.metrics_names)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Training the Generator\n",
        "            # ---------------------\n",
        "\n",
        "            z_input = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            # generate labels\n",
        "            labels_input = randint(0, self.n_classes, batch_size)\n",
        "\n",
        "            y_gan = ones((batch_size,1))\n",
        "            # nutze hier real = 0.9 statt y_gan = 1.0\n",
        "            # Train the generator (to have the discriminator label samples as real)\n",
        "            g_loss = self.combined.train_on_batch([z_input, labels_input], real)\n",
        "\n",
        "            # Print the progress and save into loss lists\n",
        "            if epoch % sample_interval == 0:\n",
        "                #print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "                print(\"%d [D loss real: %f, acc. real: %.2f%%] [D loss fake: %f, acc. fake: %.2f%%] [G loss: %f]\" % (epoch, d_loss_real[0], 100*d_loss_real[1], d_loss_fake[0], 100*d_loss_fake[1], g_loss))\n",
        "                self.disc_loss.append(d_loss[0])\n",
        "                self.gen_loss.append(g_loss)\n",
        "                self.d_acc_real.append(100*d_loss_real[1])\n",
        "                self.d_acc_fake.append(100*d_loss_fake[1])\n",
        "                self.disc_loss_real.append(d_loss_real[0])\n",
        "                self.disc_loss_fake.append(d_loss_fake[0])\n",
        "\n",
        "            ### insert: Stop if accuracy is 50%\n",
        "\n",
        "            # save models\n",
        "            if epoch != 0 and epoch % model_save_interval == 0:\n",
        "                self.save_models()\n",
        "                self.plot_loss()\n",
        "                self.plot_accuracy()\n",
        "\n",
        "        self.generate(notes)\n",
        "        self.plot_loss()\n",
        "        self.plot_accuracy()\n",
        "\n",
        "    # --------------- Source for saving the GAN model -----------------\n",
        "    # github : \"Added functionality to save/load Keras model for intermittent training\"\n",
        "    # https://github.com/eriklindernoren/Keras-GAN/pull/117/commits/4591919e57875f16e2a6b90ed728fd15070aae94\n",
        "    def load_keras_model(self, model_name):\n",
        "        json_name = os.path.join(self.model_save_dir, model_name + \".json\")\n",
        "        weights_name = os.path.join(self.model_save_dir, model_name + \".h5\")\n",
        "\n",
        "        with open(json_name, 'r') as json_file:\n",
        "            loaded_model_json = json_file.read()\n",
        "\n",
        "        loaded_model = model_from_json(loaded_model_json)\n",
        "        loaded_model.load_weights(weights_name)\n",
        "\n",
        "        return loaded_model\n",
        "\n",
        "    def save_models(self):\n",
        "        self.save_model(self.discriminator,\n",
        "                        os.path.join(self.model_save_dir, 'cdiscriminator_model'))\n",
        "        self.save_model(self.generator,\n",
        "                        os.path.join(self.model_save_dir, 'cgenerator_model'))\n",
        "        self.save_model(self.combined,\n",
        "                        os.path.join(self.model_save_dir, 'ccombined_model'))\n",
        "        print(\"model saved\")\n",
        "\n",
        "    def save_model(self, model, model_path):\n",
        "        with open(str(model_path) + '.json', 'w') as json_file:\n",
        "            json_file.write(model.to_json())\n",
        "\n",
        "        model.save_weights(str(model_path + '.h5'))\n",
        "\n",
        "    def generate(self, input_notes):\n",
        "        # Get pitch names and store in a dictionary\n",
        "        notes = input_notes\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "        num_pitches = len(pitchnames)\n",
        "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "        ##############\n",
        "        print(\"num_pitches: %d\" % len(pitchnames))\n",
        "        #############\n",
        "\n",
        "\n",
        "        ### Es fehlt: pro Emotion eine Sequenz zu generieren !!!\n",
        "        # Use random noise to generate sequences\n",
        "        # Noisevektor mit 2 Spalten\n",
        "        noise = np.random.normal(0, 1, (self.n_classes, self.latent_dim))\n",
        "        print(\"generate: noise\", noise)\n",
        "        # generate labels\n",
        "        labels = randint(0, self.n_classes, self.n_classes)\n",
        "        print(\"generate: labels\", labels)\n",
        "\n",
        "\n",
        "        # specify labels\n",
        "\n",
        "        labels = asarray([x for _ in range(self.n_classes) for x in range(self.n_classes)])\n",
        "        print(\"neue labels:\", labels)\n",
        "        # generate sequence\n",
        "        predictions = self.generator.predict([noise, labels])\n",
        "        print(\"len predict\", len(predictions))\n",
        "        #print(\"pred1:\", predictions[0])\n",
        "        #print(\"pred2:\", predictions[1])\n",
        "\n",
        "        ##############\n",
        "        # model.predict() aus Keras API: Generates output predictions for the input samples.\n",
        "        # input here: noise\n",
        "        # output: Numpy array(s) of predictions.\n",
        "        # len(predictions[0]) = 100\n",
        "        ##############\n",
        "\n",
        "        # Turn noise vector into notes und chords\n",
        "        count = 0\n",
        "        for pred in predictions:\n",
        "            pred_notes = [x * (int(num_pitches / 2)) + (num_pitches / 2) for x in pred]  # *242+242\n",
        "        ##############\n",
        "            print(\"pred_notes before:\")\n",
        "            print(pred_notes)\n",
        "        ##############\n",
        "            pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
        "        ##############\n",
        "            print(\"pred_notes after:\")\n",
        "            print(pred_notes)\n",
        "        ##############\n",
        "\n",
        "        # create and save new midi file\n",
        "            create_midi(pred_notes, 'cgan_final_'+str(count))\n",
        "            count+=1\n",
        "\n",
        "\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss_real, c='orange')\n",
        "        plt.plot(self.disc_loss_fake, c='green')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"CGAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator real', 'Discriminator fake', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('CGAN_Loss_per_Epoch.png', transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "    def plot_accuracy(self):\n",
        "        plt.plot(self.d_acc_real, c='orange')\n",
        "        plt.plot(self.d_acc_fake, c='green')\n",
        "        plt.title(\"CGAN Accuracy\")\n",
        "        plt.legend(['Acc real', 'Acc fake'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy in %')\n",
        "        plt.savefig('CGAN_Accuracy_per_Epoch.png', transparent=True)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ---------------\n",
        "    # ohne Speicherung\n",
        "    # ---------------\n",
        "    # gan = GAN(rows=100)\n",
        "    # gan.train(epochs=2, batch_size=32, sample_interval=1)\n",
        "    # ---------------\n",
        "    # mit Speicherung\n",
        "    # ---------------\n",
        "    ### Schreib dir dazu, mit welchen Konfigurationen du dein GAN startest!\n",
        "    #5000 Epochen, kein label Smoothing, big Corpus\n",
        "    gan = GAN(rows=100, load_model=False)\n",
        "    gan.train(epochs=3, batch_size=32,\n",
        "              sample_interval=1, model_save_interval=100)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 50)        100         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1, 100)       5100        embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 100, 1)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 100, 1)       0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 100, 2)       0           input_2[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 100, 512)     1054720     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 1024)         4198400     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          524800      bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          131328      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            257         leaky_re_lu_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 5,914,705\n",
            "Trainable params: 5,914,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 50)        100         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1, 1)         51          embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1)            0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1001)         0           input_4[0][0]                    \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          256512      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 256)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 256)          1024        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 512)          131584      batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 512)          0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 512)          2048        leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1024)         525312      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 1024)         0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1024)         4096        leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 100)          102500      batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 100, 1)       0           dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,023,227\n",
            "Trainable params: 1,019,643\n",
            "Non-trainable params: 3,584\n",
            "__________________________________________________________________________________________________\n",
            "network output before categorical []\n",
            "network_input []\n",
            "0\n",
            "network output before categorical []\n",
            "network_input []\n",
            "0\n",
            "x_labels []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6b51189e089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     gan.train(epochs=3, batch_size=32,\n\u001b[0;32m--> 655\u001b[0;31m               sample_interval=1, model_save_interval=100)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-e6b51189e089>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval, model_save_interval)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mx_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x_labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len xtrain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n\u001b[1;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2668\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 400,
              "height": 261
            },
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}